# ML Model Builder - Backend

ğŸš€ A comprehensive Flask-based REST API for training, storing, and serving machine learning models with automatic algorithm selection.

## âœ¨ Features

### Core Capabilities
- **ğŸ¤– Automated Model Training**: Train classification and regression models with multiple algorithms
- **ğŸ† Automatic Algorithm Selection**: Intelligently selects the best performing algorithm
- **ğŸ’¾ Model Persistence**: Store models in MySQL database with full metadata
- **ğŸ”® REST API for Predictions**: Serve predictions via simple HTTP endpoints
- **ğŸ“Š Batch Processing**: Make predictions on multiple samples simultaneously
- **ğŸ“ˆ Model Management**: Full CRUD operations for trained models
- **ğŸ“‹ Training History**: Track all algorithm performances
- **ğŸ”„ Prediction Logging**: Audit trail of all predictions

### Supported Algorithms
- **Classification**: Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, SVM, Naive Bayes, KNN
- **Regression**: Linear Regression, Ridge, Lasso, Decision Tree, Random Forest, Gradient Boosting, SVM, KNN

## ğŸš€ Quick Start

### Prerequisites
```bash
- Python 3.8+
- MySQL Server 5.7+
- pip
```

### Installation

1. **Clone/Navigate to backend**
   ```bash
   cd back-end
   ```

2. **Create MySQL database**
   ```bash
   mysql -u root -p -e "CREATE DATABASE ml_models;"
   ```

3. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env with your MySQL credentials
   ```

4. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

5. **Run the server**
   ```bash
   python app.py
   ```

The API will be available at `http://localhost:5000`

## ğŸ”Œ API Endpoints Overview

### Training
- `POST /api/train` - Train and save a new model

### Model Management
- `GET /api/models` - List all models
- `GET /api/models/<id>` - Get model details
- `PUT /api/models/<id>` - Update model metadata
- `DELETE /api/models/<id>` - Delete a model

### Predictions
- `POST /api/models/<id>/predict` - Single prediction
- `POST /api/models/<id>/batch-predict` - Batch predictions

### Utilities
- `POST /api/parse-csv` - Parse and analyze CSV
- `GET /api/health` - Health check

## ğŸ“Š Project Structure

```
back-end/
â”œâ”€â”€ app.py                      # Main Flask application
â”œâ”€â”€ config.py                   # Configuration
â”œâ”€â”€ database.py                 # Database management
â”œâ”€â”€ model_serializer.py         # Model storage & preprocessing
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example               # Environment template
â”œâ”€â”€ API_DOCUMENTATION.md       # API reference
â”œâ”€â”€ SETUP_GUIDE.md            # Setup instructions
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ example_client.py         # Python client example
â”œâ”€â”€ models/                   # Stored models (auto-created)
â””â”€â”€ algorithms/               # Reference implementations
```

## ğŸ’» Usage Example

```python
from example_client import MLModelClient

# Initialize client
client = MLModelClient("http://localhost:5000")

# Prepare CSV data
csv_data = """age,income,approved
25,30000,0
35,60000,1
45,80000,1"""

# Train model
result = client.train_model(
    model_name="Loan Model",
    model_type="classification",
    csv_data=csv_data,
    input_features=["age", "income"],
    output_feature="approved"
)

model_id = result["model_id"]

# Make prediction
prediction = client.predict(model_id, {
    "age": 35,
    "income": 65000
})

print(f"Prediction: {prediction['prediction']}")
```

## ğŸ—„ï¸ Database Schema

### Models Table
Stores trained model information with metrics, features, and file paths.

### Training Results Table
Tracks performance of all algorithms tested for each model.

### Predictions Table
Audit trail of all predictions made with the models.

See [database.py](./database.py) for detailed schema.

## ğŸ”§ Configuration

### Environment Variables (.env)
```env
# Database
DB_HOST=localhost
DB_USER=root
DB_PASSWORD=password
DB_NAME=ml_models
DB_PORT=3306

# Flask
FLASK_DEBUG=True
SECRET_KEY=your-secret-key

# Storage
MODEL_STORAGE_PATH=./models
```

## ğŸ“¦ Dependencies

- **Flask 3.0.0** - Web framework
- **pandas 2.1.4** - Data manipulation
- **scikit-learn 1.3.2** - ML algorithms
- **mysql-connector-python 8.2.0** - MySQL connection
- **numpy 1.26.2** - Numerical computing
- **python-dotenv 1.0.0** - Environment management

## ğŸ¯ API Workflow Example

### 1. Parse CSV
```bash
curl -X POST http://localhost:5000/api/parse-csv \
  -H "Content-Type: application/json" \
  -d '{"csv_data": "age,income\n25,30000\n35,60000"}'
```

### 2. Train Model
```bash
curl -X POST http://localhost:5000/api/train \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "My Model",
    "model_type": "classification",
    "csv_data": "...",
    "input_features": ["age", "income"],
    "output_feature": "target"
  }'
```
```

```
## ğŸ”„ Model Serialization

Models are stored as:
- **Model file** (`model_{id}_{name}.pkl`) - Trained sklearn model
- **Metadata file** (`metadata_{id}.json`) - Preprocessing info, encoders, scalers

This allows models to be:
- Loaded on demand for predictions
- Versioned and tracked
- Shared across servers
- Archived and restored

## ğŸ“ Example Workflows

### Classification Task
1. Upload loan approval dataset
2. Select features and target column
3. API trains multiple classifiers
4. Best model (e.g., Random Forest) selected automatically
5. Deployed for approval predictions

### Regression Task
1. Upload house price dataset
2. Train regression models
3. Best model selected (e.g., Gradient Boosting)
4. Use for price predictions

## ğŸ¤ Integration with Frontend

The frontend sends:
1. CSV data as string
2. Feature and target columns
3. Model type (classification/regression)

The backend returns:
1. Model ID
2. Algorithm results
3. Performance metrics
4. Prediction endpoint URL